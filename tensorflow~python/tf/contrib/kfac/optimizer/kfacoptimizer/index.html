
<!DOCTYPE HTML>

<html lang="en">

<head>
  <meta charset="utf-8">
  <title>contrib.kfac.optimizer.KfacOptimizer - TensorFlow Python - W3cubDocs</title>
  
  <meta name="description" content=" Inherits From&#58; GradientDescentOptimizer ">
  <meta name="keywords" content="tf, contrib, kfac, optimizer, kfacoptimizer, -, tensorflow, python, tensorflow~python">
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="mobile-web-app-capable" content="yes">
  
  <link rel="canonical" href="http://docs.w3cub.com/tensorflow~python/tf/contrib/kfac/optimizer/kfacoptimizer/">
  <link href="/favicon.png" rel="icon">
  <link type="text/css" rel="stylesheet" href="/assets/application-e0f881b0fce8cbe96f33dadc355b7159d5d2912911231446d60eb8f9caffa802.css">
  <script type="text/javascript" src="/assets/application-8fc46316434047265cd383f02b8e7f434ed7dec4a59f920dd633deef8d488d1b.js"></script>
  <script src="/json/tensorflow~python.js"></script>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-71174418-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body>
	<div class="_app">
	<header class="_header">
  
  <form class="_search">
    <input type="search" class="_search-input" placeholder="Search&hellip;" autocomplete="off" autocapitalize="off" autocorrect="off" spellcheck="false" maxlength="20">
    <a class="_search-clear"></a>
    <div class="_search-tag"></div>
  </form>
  
  <a class="_home-link" href="/" ></a>
  <a class="_menu-link"></a>
  <h1 class="_logo">
    <a href="/" class="_nav-link" title="API Documentation Browser">W3cubDocs</a>
  </h1>
  
  <span class="_logo-sub-nav">/</span><span class="_logo-sub-nav"><a href="/tensorflow~python/" class="_nav-link" title="" style="margin-left:0;">TensorFlow Python</a></span>
  
  <nav class="_nav">
    <a href="/app/" class="_nav-link ">App</a>
    <a href="/about/" class="_nav-link ">About</a>
  </nav>
</header>
	<section class="_sidebar">
		<div class="_list">
			
		</div>
	</section>
	<section class="_container ">
		<div class="_content">
			<div class="_page _tensorflow">
				
<h1 itemprop="name" class="devsite-page-title"> tf.contrib.kfac.optimizer.KfacOptimizer </h1>     <div itemscope itemtype="http://developers.google.com/ReferenceObject"> <meta itemprop="name" content="tf.contrib.kfac.optimizer.KfacOptimizer"> <meta itemprop="path" content="r1.4"> <meta itemprop="property" content="damping"> <meta itemprop="property" content="variables"> <meta itemprop="property" content="__init__"> <meta itemprop="property" content="apply_gradients"> <meta itemprop="property" content="compute_gradients"> <meta itemprop="property" content="get_name"> <meta itemprop="property" content="get_slot"> <meta itemprop="property" content="get_slot_names"> <meta itemprop="property" content="minimize"> <meta itemprop="property" content="GATE_GRAPH"> <meta itemprop="property" content="GATE_NONE"> <meta itemprop="property" content="GATE_OP"> </div> <h2 id="class_kfacoptimizer">Class <code>KfacOptimizer</code>
</h2> <p>Inherits From: <a href="../../../../train/gradientdescentoptimizer/"><code>GradientDescentOptimizer</code></a></p> <p>Defined in <a href="https://www.github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/contrib/kfac/python/ops/optimizer.py" target="_blank"><code>tensorflow/contrib/kfac/python/ops/optimizer.py</code></a>.</p> <p>The KFAC Optimizer (https://arxiv.org/abs/1503.05671).</p> <h2 id="properties">Properties</h2> <h3 id="damping"><code>damping</code></h3> <h3 id="variables"><code>variables</code></h3> <h2 id="methods">Methods</h2> <h3 id="__init__"><code>__init__</code></h3> <pre class="prettyprint lang-python" data-language="python">__init__(
    learning_rate,
    cov_ema_decay,
    damping,
    layer_collection,
    momentum=0.0,
    momentum_type='regular',
    norm_constraint=None,
    name='KFAC'
)
</pre> <p>Initializes the KFAC optimizer with the given settings.</p> <h4 id="args">Args:</h4> <ul> <li>
<b><code>learning_rate</code></b>: The base learning rate for the optimizer. Should probably be set to 1.0 when using momentum_type = 'qmodel', but can still be set lowered if desired (effectively lowering the trust in the quadratic model.)</li> <li>
<b><code>cov_ema_decay</code></b>: The decay factor used when calculating the covariance estimate moving averages.</li> <li>
<b><code>damping</code></b>: The damping factor used to stabilize training due to errors in the local approximation with the Fisher information matrix, and to regularize the update direction by making it closer to the gradient. (Higher damping means the update looks more like a standard gradient update - see Tikhonov regularization.)</li> <li>
<b><code>layer_collection</code></b>: The layer collection object, which holds the fisher blocks, kronecker factors, and losses associated with the graph. The layer_collection cannot be modified after KfacOptimizer's initialization.</li> <li>
<b><code>momentum</code></b>: The momentum value for this optimizer. Only applies when momentum_type is 'regular' or 'adam'. (Default: 0)</li> <li>
<b><code>momentum_type</code></b>: The type of momentum to use in this optimizer, one of 'regular', 'adam', or 'qmodel'. (Default: 'regular')</li> <li>
<b><code>norm_constraint</code></b>: float or Tensor. If specified, the update is scaled down so that its approximate squared Fisher norm v^T F v is at most the specified value. May only be used with momentum type 'regular'. (Default: None)</li> <li>
<b><code>name</code></b>: The name for this optimizer. (Default: 'KFAC')</li> </ul> <h4 id="raises">Raises:</h4> <ul> <li>
<b><code>ValueError</code></b>: If the momentum type is unsupported.</li> <li>
<b><code>ValueError</code></b>: If clipping is used with momentum type other than 'regular'.</li> <li>
<b><code>ValueError</code></b>: If no losses have been registered with layer_collection.</li> <li>
<b><code>ValueError</code></b>: If momentum is non-zero and momentum_type is not 'regular' or 'adam'.</li> </ul> <h3 id="apply_gradients"><code>apply_gradients</code></h3> <pre class="prettyprint lang-python" data-language="python">apply_gradients(
    grads_and_vars,
    *args,
    **kwargs
)
</pre> <p>Applies gradients to variables.</p> <h4 id="args_1">Args:</h4> <ul> <li>
<b><code>grads_and_vars</code></b>: List of (gradient, variable) pairs.</li> <li>
<b><code>*args</code></b>: Additional arguments for super.apply_gradients.</li> <li>
<b><code>**kwargs</code></b>: Additional keyword arguments for super.apply_gradients.</li> </ul> <h4 id="returns">Returns:</h4> <p>An <code>Operation</code> that applies the specified gradients.</p> <h3 id="compute_gradients"><code>compute_gradients</code></h3> <pre class="prettyprint lang-python" data-language="python">compute_gradients(
    loss,
    var_list=None,
    gate_gradients=GATE_OP,
    aggregation_method=None,
    colocate_gradients_with_ops=False,
    grad_loss=None
)
</pre> <p>Compute gradients of <code>loss</code> for the variables in <code>var_list</code>.</p> <p>This is the first part of <code>minimize()</code>. It returns a list of (gradient, variable) pairs where "gradient" is the gradient for "variable". Note that "gradient" can be a <code>Tensor</code>, an <code>IndexedSlices</code>, or <code>None</code> if there is no gradient for the given variable.</p> <h4 id="args_2">Args:</h4> <ul> <li>
<b><code>loss</code></b>: A Tensor containing the value to minimize.</li> <li>
<b><code>var_list</code></b>: Optional list or tuple of <code>tf.Variable</code> to update to minimize <code>loss</code>. Defaults to the list of variables collected in the graph under the key <code>GraphKey.TRAINABLE_VARIABLES</code>.</li> <li>
<b><code>gate_gradients</code></b>: How to gate the computation of gradients. Can be <code>GATE_NONE</code>, <code>GATE_OP</code>, or <code>GATE_GRAPH</code>.</li> <li>
<b><code>aggregation_method</code></b>: Specifies the method used to combine gradient terms. Valid values are defined in the class <code>AggregationMethod</code>.</li> <li>
<b><code>colocate_gradients_with_ops</code></b>: If True, try colocating gradients with the corresponding op.</li> <li>
<b><code>grad_loss</code></b>: Optional. A <code>Tensor</code> holding the gradient computed for <code>loss</code>.</li> </ul> <h4 id="returns_1">Returns:</h4> <p>A list of (gradient, variable) pairs. Variable is always present, but gradient can be <code>None</code>.</p> <h4 id="raises_1">Raises:</h4> <ul> <li>
<b><code>TypeError</code></b>: If <code>var_list</code> contains anything else than <code>Variable</code> objects.</li> <li>
<b><code>ValueError</code></b>: If some arguments are invalid.</li> </ul> <h3 id="get_name"><code>get_name</code></h3> <pre class="prettyprint lang-python" data-language="python">get_name()
</pre> <h3 id="get_slot"><code>get_slot</code></h3> <pre class="prettyprint lang-python" data-language="python">get_slot(
    var,
    name
)
</pre> <p>Return a slot named <code>name</code> created for <code>var</code> by the Optimizer.</p> <p>Some <code>Optimizer</code> subclasses use additional variables. For example <code>Momentum</code> and <code>Adagrad</code> use variables to accumulate updates. This method gives access to these <code>Variable</code> objects if for some reason you need them.</p> <p>Use <code>get_slot_names()</code> to get the list of slot names created by the <code>Optimizer</code>.</p> <h4 id="args_3">Args:</h4> <ul> <li>
<b><code>var</code></b>: A variable passed to <code>minimize()</code> or <code>apply_gradients()</code>.</li> <li>
<b><code>name</code></b>: A string.</li> </ul> <h4 id="returns_2">Returns:</h4> <p>The <code>Variable</code> for the slot if it was created, <code>None</code> otherwise.</p> <h3 id="get_slot_names"><code>get_slot_names</code></h3> <pre class="prettyprint lang-python" data-language="python">get_slot_names()
</pre> <p>Return a list of the names of slots created by the <code>Optimizer</code>.</p> <p>See <code>get_slot()</code>.</p> <h4 id="returns_3">Returns:</h4> <p>A list of strings.</p> <h3 id="minimize"><code>minimize</code></h3> <pre class="prettyprint lang-python" data-language="python">minimize(
    *args,
    **kwargs
)
</pre> <h2 id="class_members">Class Members</h2> <h3 id="GATE_GRAPH"><code>GATE_GRAPH</code></h3> <h3 id="GATE_NONE"><code>GATE_NONE</code></h3> <h3 id="GATE_OP"><code>GATE_OP</code></h3>
<div class="_attribution">
  <p class="_attribution-p">
    Â© 2017 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/kfac/optimizer/KfacOptimizer" class="_attribution-link" target="_blank">https://www.tensorflow.org/api_docs/python/tf/contrib/kfac/optimizer/KfacOptimizer</a>
  </p>
</div>

			</div>
		</div>
	</section>

	</div>
	<svg style="display:none">
		<symbol id="icon-dir"><svg viewBox="0 0 20 20"><path d="M15 10c0 .3-.305.515-.305.515l-8.56 5.303c-.625.41-1.135.106-1.135-.67V4.853c0-.777.51-1.078 1.135-.67l8.56 5.305S15 9.702 15 10z"/></svg></symbol>
	  </svg>
</body>
</html>
